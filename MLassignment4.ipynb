{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d02a5c40-0147-4fde-b8e4-2bc2e737765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf3852f4-c9e7-440b-855e-27d4504295a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>3</td>\n",
       "      <td>£51.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>1</td>\n",
       "      <td>£53.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>1</td>\n",
       "      <td>£50.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>4</td>\n",
       "      <td>£47.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>5</td>\n",
       "      <td>£54.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>1</td>\n",
       "      <td>£22.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets of Getting Your Dream...</td>\n",
       "      <td>4</td>\n",
       "      <td>£33.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A Novel Based on the Life of...</td>\n",
       "      <td>3</td>\n",
       "      <td>£17.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the Boat: Nine Americans and Their...</td>\n",
       "      <td>4</td>\n",
       "      <td>£22.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>1</td>\n",
       "      <td>£52.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade Trilogy, #1)</td>\n",
       "      <td>2</td>\n",
       "      <td>£13.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>4</td>\n",
       "      <td>£20.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>5</td>\n",
       "      <td>£17.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little Life (Scott Pi...</td>\n",
       "      <td>5</td>\n",
       "      <td>£52.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and Start Again</td>\n",
       "      <td>5</td>\n",
       "      <td>£35.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be Your Life: Scenes from the A...</td>\n",
       "      <td>3</td>\n",
       "      <td>£57.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>1</td>\n",
       "      <td>£23.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science Fiction Stories 18...</td>\n",
       "      <td>1</td>\n",
       "      <td>£37.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>2</td>\n",
       "      <td>£51.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>2</td>\n",
       "      <td>£45.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  Rating   Price\n",
       "0                                A Light in the Attic       3  £51.77\n",
       "1                                  Tipping the Velvet       1  £53.74\n",
       "2                                          Soumission       1  £50.10\n",
       "3                                       Sharp Objects       4  £47.82\n",
       "4               Sapiens: A Brief History of Humankind       5  £54.23\n",
       "5                                     The Requiem Red       1  £22.65\n",
       "6   The Dirty Little Secrets of Getting Your Dream...       4  £33.34\n",
       "7   The Coming Woman: A Novel Based on the Life of...       3  £17.93\n",
       "8   The Boys in the Boat: Nine Americans and Their...       4  £22.60\n",
       "9                                     The Black Maria       1  £52.15\n",
       "10     Starving Hearts (Triangular Trade Trilogy, #1)       2  £13.99\n",
       "11                              Shakespeare's Sonnets       4  £20.66\n",
       "12                                        Set Me Free       5  £17.46\n",
       "13  Scott Pilgrim's Precious Little Life (Scott Pi...       5  £52.29\n",
       "14                          Rip it Up and Start Again       5  £35.02\n",
       "15  Our Band Could Be Your Life: Scenes from the A...       3  £57.25\n",
       "16                                               Olio       1  £23.88\n",
       "17  Mesaerion: The Best Science Fiction Stories 18...       1  £37.59\n",
       "18                       Libertarianism for Beginners       2  £51.33\n",
       "19                            It's Only the Himalayas       2  £45.17"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://books.toscrape.com/'\n",
    "r=requests.get(url)\n",
    "soup=BeautifulSoup(r.content,'html.parser')\n",
    "data=[]\n",
    "nums = {\"One\":1, \"Two\":2, \"Three\":3, \"Four\":4, \"Five\":5}\n",
    "for d in soup.find_all('li',attrs={'class':\"col-xs-6 col-sm-4 col-md-3 col-lg-3\"}):\n",
    "    img = d.find('img')\n",
    "    title = img['alt']\n",
    "    stars = d.find('p',attrs={'class':'star-rating'})\n",
    "    stars = nums[stars['class'][1]]\n",
    "    price = d.find('p',attrs={'class':'price_color'}).text\n",
    "    availability = d.find('p',attrs={'class':'instock availability'}).text\n",
    "    obj = {\"Title\":title, \"Rating\":stars, \"Price\":price}\n",
    "    data.append(obj)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842f90c7-d9e7-46ff-9783-a5c229f6e4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\parth\\anaconda3\\lib\\site-packages (4.36.0)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.30.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (0.31.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\parth\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35ccb0ae-fcd8-4b9a-9891-2d5c405f79b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "“I have not failed. I've just found 10,000 ways that won't work.”\n",
      "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "“A day without sunshine is like, you know, night.”\n"
     ]
    }
   ],
   "source": [
    "def get_default_chrome_options():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    return options\n",
    "\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "driver = webdriver.Chrome() \n",
    "driver.get(\"https://quotes.toscrape.com/js/\") \n",
    "options = get_default_chrome_options()\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "quotes = driver.find_elements(By.CLASS_NAME, \"text\") \n",
    "data = []\n",
    "for q in quotes: \n",
    "    print(q.text)\n",
    "    data.append(q.text)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce8601e7-aaff-4a6d-89d6-69be45b6cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\parth\\anaconda3\\lib\\site-packages (4.36.0)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.30.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (0.31.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\parth\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\parth\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\parth\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\parth\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\parth\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.14.0)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1c92868-e557-4094-9df6-a2e242066b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No next button found — reached last page.\n",
      "Saved 100 rows to quotes.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, TimeoutException, StaleElementReferenceException\n",
    ")\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import csv\n",
    "\n",
    "URL = \"https://quotes.toscrape.com/js/\"\n",
    "\n",
    "def create_driver(headless=False):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")  # newer headless mode\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=opts)\n",
    "    return driver\n",
    "\n",
    "def scrape_all_pages():\n",
    "    driver = create_driver(headless=True)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    driver.get(URL)\n",
    "\n",
    "    results = []\n",
    "    page_index = 1\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Wait for quotes to be present on the current page\n",
    "            wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"quote\")))\n",
    "        except TimeoutException:\n",
    "            print(f\"Timed out waiting for quotes on page {page_index}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Collect all quotes on the page\n",
    "        quotes = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
    "        for q in quotes:\n",
    "            try:\n",
    "                text = q.find_element(By.CLASS_NAME, \"text\").text.strip()\n",
    "                author = q.find_element(By.CLASS_NAME, \"author\").text.strip()\n",
    "                tags = [t.text.strip() for t in q.find_elements(By.CLASS_NAME, \"tag\")]\n",
    "                results.append({\"quote\": text, \"author\": author, \"tags\": \";\".join(tags)})\n",
    "            except (StaleElementReferenceException, NoSuchElementException):\n",
    "                # element disappeared between find and access — skip it\n",
    "                continue\n",
    "\n",
    "        # Try to find and click the Next button. If not found, we're done.\n",
    "        try:\n",
    "            next_link = driver.find_element(By.CSS_SELECTOR, \"li.next a\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"No next button found — reached last page.\")\n",
    "            break\n",
    "\n",
    "        # Click and wait for URL to change (safe navigation)\n",
    "        prev_url = driver.current_url\n",
    "        try:\n",
    "            next_link.click()\n",
    "        except Exception:\n",
    "            # fallback to JS click if normal click fails\n",
    "            driver.execute_script(\"arguments[0].click();\", next_link)\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.url_changes(prev_url))\n",
    "        except TimeoutException:\n",
    "            # URL didn't change within timeout, but maybe content changed — attempt to continue\n",
    "            print(f\"Warning: URL didn't change after clicking Next on page {page_index}.\")\n",
    "        page_index += 1\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "def save_to_csv(rows, filename=\"quotes.csv\"):\n",
    "    keys = [\"quote\", \"author\", \"tags\"]\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"Saved {len(rows)} rows to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_all_pages()\n",
    "    save_to_csv(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57e1c2bb-e34e-4fda-9661-f2399712314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMDB Top 250 Movies Scraper\n",
      "============================================================\n",
      "Initializing Chrome driver...\n",
      "Navigating to https://www.imdb.com/chart/top/...\n",
      "Page loaded successfully. Extracting movie data...\n",
      "Found 250 movies. Processing...\n",
      "Processed 50 movies...\n",
      "Processed 100 movies...\n",
      "Processed 150 movies...\n",
      "Processed 200 movies...\n",
      "Processed 250 movies...\n",
      "Successfully extracted data for 250 movies.\n",
      "\n",
      "Data saved to 'imdb_top250.csv'\n",
      "\n",
      "First 10 movies:\n",
      " Rank                                       Movie Title Year of Release IMDB Rating\n",
      "    1                          The Shawshank Redemption            1994         9.3\n",
      "    2                                     The Godfather            1972         9.2\n",
      "    3                                   The Dark Knight            2008         9.1\n",
      "    4                             The Godfather Part II            1974         9.0\n",
      "    5                                      12 Angry Men            1957         9.0\n",
      "    6     The Lord of the Rings: The Return of the King            2003         9.0\n",
      "    7                                  Schindler's List            1993         9.0\n",
      "    8 The Lord of the Rings: The Fellowship of the Ring            2001         8.9\n",
      "    9                                      Pulp Fiction            1994         8.8\n",
      "   10                    The Good, the Bad and the Ugly            1966         8.8\n",
      "\n",
      "Total movies scraped: 250\n",
      "Average rating: 8.31\n",
      "\n",
      "Browser closed.\n",
      "\n",
      "✓ Scraping completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_imdb_top250():\n",
    "    \"\"\"\n",
    "    Scrapes IMDB Top 250 Movies list and saves to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # Run in background\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "    \n",
    "    # Initialize the driver\n",
    "    print(\"Initializing Chrome driver...\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        # Navigate to IMDB Top 250\n",
    "        url = \"https://www.imdb.com/chart/top/\"\n",
    "        print(f\"Navigating to {url}...\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"ipc-metadata-list\")))\n",
    "        \n",
    "        print(\"Page loaded successfully. Extracting movie data...\")\n",
    "        \n",
    "        # Lists to store data\n",
    "        ranks = []\n",
    "        titles = []\n",
    "        years = []\n",
    "        ratings = []\n",
    "        \n",
    "        # Find all movie items\n",
    "        movie_items = driver.find_elements(By.CSS_SELECTOR, \"li.ipc-metadata-list-summary-item\")\n",
    "        \n",
    "        print(f\"Found {len(movie_items)} movies. Processing...\")\n",
    "        \n",
    "        for idx, item in enumerate(movie_items, 1):\n",
    "            try:\n",
    "                # Extract rank\n",
    "                rank = idx\n",
    "                \n",
    "                # Extract title\n",
    "                title_element = item.find_element(By.CSS_SELECTOR, \"h3.ipc-title__text\")\n",
    "                title_text = title_element.text\n",
    "                # Remove the rank prefix (e.g., \"1. The Shawshank Redemption\" -> \"The Shawshank Redemption\")\n",
    "                title = title_text.split('. ', 1)[1] if '. ' in title_text else title_text\n",
    "                \n",
    "                # Extract year\n",
    "                metadata = item.find_elements(By.CSS_SELECTOR, \"span.cli-title-metadata-item\")\n",
    "                year = metadata[0].text if metadata else \"N/A\"\n",
    "                \n",
    "                # Extract rating\n",
    "                rating_element = item.find_element(By.CSS_SELECTOR, \"span.ipc-rating-star--imdb\")\n",
    "                rating_text = rating_element.get_attribute(\"aria-label\")\n",
    "                # Extract numeric rating from \"IMDb rating: 9.3\"\n",
    "                rating = rating_text.split(\": \")[1] if \": \" in rating_text else \"N/A\"\n",
    "                \n",
    "                # Append to lists\n",
    "                ranks.append(rank)\n",
    "                titles.append(title)\n",
    "                years.append(year)\n",
    "                ratings.append(rating)\n",
    "                \n",
    "                # Print progress every 50 movies\n",
    "                if idx % 50 == 0:\n",
    "                    print(f\"Processed {idx} movies...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing movie {idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully extracted data for {len(titles)} movies.\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Rank': ranks,\n",
    "            'Movie Title': titles,\n",
    "            'Year of Release': years,\n",
    "            'IMDB Rating': ratings\n",
    "        })\n",
    "        \n",
    "        # Export to CSV\n",
    "        csv_filename = 'imdb_top250.csv'\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        print(f\"\\nData saved to '{csv_filename}'\")\n",
    "        \n",
    "        # Display first 10 rows\n",
    "        print(\"\\nFirst 10 movies:\")\n",
    "        print(df.head(10).to_string(index=False))\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(f\"\\nTotal movies scraped: {len(df)}\")\n",
    "        print(f\"Average rating: {df['IMDB Rating'].astype(float).mean():.2f}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "        print(\"\\nBrowser closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"IMDB Top 250 Movies Scraper\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = scrape_imdb_top250()\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"\\n✓ Scraping completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n✗ Scraping failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06729978-1d6f-43e0-b1d1-257619c8795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Weather Information Scraper - TimeAndDate.com\n",
      "============================================================\n",
      "\n",
      "Initializing Chrome driver...\n",
      "Navigating to https://www.timeanddate.com/weather/...\n",
      "Waiting for page to load completely...\n",
      "Page loaded. Extracting weather data...\n",
      "Found 48 table rows. Processing...\n",
      "Processed 25 cities...\n",
      "Processed 50 cities...\n",
      "Processed 75 cities...\n",
      "Processed 100 cities...\n",
      "Processed 125 cities...\n",
      "\n",
      "Successfully extracted data for 141 cities.\n",
      "\n",
      "Data saved to 'weather.csv'\n",
      "\n",
      "Sample Weather Data (first 20 cities):\n",
      "     City Name Temperature Weather Condition\n",
      "         Accra        25°C        Light rain\n",
      "        Dublin        10°C    Passing clouds\n",
      "       Nairobi        17°C     Broken clouds\n",
      "   Addis Ababa        12°C  Scattered clouds\n",
      "      Edmonton        -3°C             Clear\n",
      "        Nassau        26°C    Passing clouds\n",
      "      Adelaide        29°C   Pleasantly warm\n",
      "     Frankfurt        13°C      Partly sunny\n",
      "     New Delhi        30°C             Sunny\n",
      "       Algiers        22°C      Partly sunny\n",
      "Guatemala City        16°C     Partly cloudy\n",
      "   New Orleans        23°C             Clear\n",
      "        Almaty         8°C  Scattered clouds\n",
      "       Halifax         8°C    Passing clouds\n",
      "      New York        15°C     Mostly cloudy\n",
      "         Amman        17°C    Passing clouds\n",
      "         Hanoi        31°C     Broken clouds\n",
      "          Oslo         4°C               Fog\n",
      "     Amsterdam        13°C           Drizzle\n",
      "        Harare        24°C             Sunny\n",
      "\n",
      "============================================================\n",
      "Total cities scraped: 141\n",
      "Cities with temperature data: 141\n",
      "Cities with weather condition data: 141\n",
      "\n",
      "Unique weather conditions found: 23\n",
      "Examples: Light rain, Passing clouds, Broken clouds, Scattered clouds, Clear, Pleasantly warm, Partly sunny, Sunny, Partly cloudy, Mostly cloudy\n",
      "\n",
      "Closing browser...\n",
      "Browser closed.\n",
      "\n",
      "✓ Scraping completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "def scrape_weather_data():\n",
    "    \"\"\"\n",
    "    Scrapes weather information for top world cities from timeanddate.com\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure Chrome options\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument('--headless')  # Uncomment for headless mode\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    # Initialize the driver\n",
    "    print(\"Initializing Chrome driver...\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    try:\n",
    "        # Navigate to the weather page\n",
    "        url = \"https://www.timeanddate.com/weather/\"\n",
    "        print(f\"Navigating to {url}...\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for page to load completely\n",
    "        print(\"Waiting for page to load completely...\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Scroll down to load all content\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        print(\"Page loaded. Extracting weather data...\")\n",
    "        \n",
    "        # Lists to store data\n",
    "        cities = []\n",
    "        temperatures = []\n",
    "        conditions = []\n",
    "        \n",
    "        # Find all table rows\n",
    "        all_rows = driver.find_elements(By.CSS_SELECTOR, \"table tr\")\n",
    "        \n",
    "        print(f\"Found {len(all_rows)} table rows. Processing...\")\n",
    "        \n",
    "        for row in all_rows:\n",
    "            try:\n",
    "                # Get all cells in this row\n",
    "                all_cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                \n",
    "                if len(all_cells) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # Each row contains multiple cities\n",
    "                # Pattern: city link, time, weather+temp, wind | city link, time, weather+temp, wind | ...\n",
    "                # We need to process cells in groups\n",
    "                \n",
    "                i = 0\n",
    "                while i < len(all_cells):\n",
    "                    try:\n",
    "                        # Check if this cell contains a city link\n",
    "                        city_links = all_cells[i].find_elements(By.CSS_SELECTOR, \"a[href*='/weather/']\")\n",
    "                        \n",
    "                        if not city_links:\n",
    "                            i += 1\n",
    "                            continue\n",
    "                        \n",
    "                        city_name = city_links[0].text.strip()\n",
    "                        \n",
    "                        if not city_name or len(city_name) < 2:\n",
    "                            i += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Next cell should be time (class=\"r\"), skip it\n",
    "                        # The cell after that should contain weather image and temperature\n",
    "                        temperature = \"N/A\"\n",
    "                        condition = \"N/A\"\n",
    "                        \n",
    "                        # Look ahead for the weather data cell (usually 2 cells ahead)\n",
    "                        if i + 2 < len(all_cells):\n",
    "                            weather_cell = all_cells[i + 2]\n",
    "                            \n",
    "                            # Extract weather condition from image alt attribute\n",
    "                            try:\n",
    "                                weather_img = weather_cell.find_element(By.TAG_NAME, \"img\")\n",
    "                                condition = weather_img.get_attribute(\"alt\").strip()\n",
    "                                \n",
    "                                # Remove any extra text after period (e.g., \"Snow flurries. Overcast. Cold.\" -> \"Snow flurries\")\n",
    "                                if '.' in condition:\n",
    "                                    condition = condition.split('.')[0].strip()\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        # Temperature is in the next cell (class=\"rbi\")\n",
    "                        if i + 3 < len(all_cells):\n",
    "                            temp_cell = all_cells[i + 3]\n",
    "                            cell_text = temp_cell.text.strip()\n",
    "                            \n",
    "                            # Extract temperature (format like \"-4 °C\" or \"25 °C\")\n",
    "                            # Remove &nbsp; and extract number with degree symbol\n",
    "                            cell_text = cell_text.replace('\\xa0', ' ')  # Replace &nbsp;\n",
    "                            temp_match = re.search(r'(-?\\d+)\\s*°[CF]?', cell_text)\n",
    "                            if temp_match:\n",
    "                                temperature = temp_match.group(1) + \"°C\"\n",
    "                        \n",
    "                        # Only add if we got at least temperature or condition\n",
    "                        if temperature != \"N/A\" or condition != \"N/A\":\n",
    "                            cities.append(city_name)\n",
    "                            temperatures.append(temperature)\n",
    "                            conditions.append(condition)\n",
    "                            \n",
    "                            if len(cities) % 25 == 0:\n",
    "                                print(f\"Processed {len(cities)} cities...\")\n",
    "                        \n",
    "                        # Move to next potential city (usually 4 cells ahead: city, time, weather, wind)\n",
    "                        i += 4\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nSuccessfully extracted data for {len(cities)} cities.\")\n",
    "        \n",
    "        if not cities:\n",
    "            print(\"\\n⚠ No weather data could be extracted.\")\n",
    "            print(\"The page structure may have changed or dynamic content didn't load.\")\n",
    "            return None\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'City Name': cities,\n",
    "            'Temperature': temperatures,\n",
    "            'Weather Condition': conditions\n",
    "        })\n",
    "        \n",
    "        # Remove duplicates (keep first occurrence)\n",
    "        df = df.drop_duplicates(subset=['City Name'], keep='first')\n",
    "        \n",
    "        # Export to CSV\n",
    "        csv_filename = 'weather.csv'\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        print(f\"\\nData saved to '{csv_filename}'\")\n",
    "        \n",
    "        # Display sample results\n",
    "        print(\"\\nSample Weather Data (first 20 cities):\")\n",
    "        print(df.head(20).to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Total cities scraped: {len(df)}\")\n",
    "        print(f\"Cities with temperature data: {len(df[df['Temperature'] != 'N/A'])}\")\n",
    "        print(f\"Cities with weather condition data: {len(df[df['Weather Condition'] != 'N/A'])}\")\n",
    "        \n",
    "        # Show some example weather conditions\n",
    "        unique_conditions = df[df['Weather Condition'] != 'N/A']['Weather Condition'].unique()\n",
    "        print(f\"\\nUnique weather conditions found: {len(unique_conditions)}\")\n",
    "        print(f\"Examples: {', '.join(list(unique_conditions)[:10])}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        print(\"\\nClosing browser...\")\n",
    "        driver.quit()\n",
    "        print(\"Browser closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Weather Information Scraper - TimeAndDate.com\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    df = scrape_weather_data()\n",
    "    \n",
    "    if df is not None and len(df) > 0:\n",
    "        print(\"\\n✓ Scraping completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n✗ Scraping failed. Please check the error messages above.\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Make sure you have a stable internet connection\")\n",
    "        print(\"2. The website may have anti-scraping measures\")\n",
    "        print(\"3. Try running without headless mode to see what's happening\")\n",
    "        print(\"4. Check if the website structure has changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505b841-61a4-49e6-9e80-b3f7afb8bd61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
